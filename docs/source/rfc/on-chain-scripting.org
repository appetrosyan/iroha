#+TITLE: Standards for on-chain scripting and smart contracts for Iroha v2.0 release
#+AUTHOR: Aleksandr Petrosyan
#+DATE: 2023-08-22
* Introduction

Many blockchains including BitCoin have adopted some on-chain
scripting capability.  Iroha is built from the ground up to support
the functionality via both the Iroha Special Instructions and at
present by supporting custom executable logic in the form of the
executor and smart contracts via the WASM standard.

Through experience and given the benefit of hindsight, we have come to
the conclusion that the standard has certain limitations and that so
far the team has not over committed to WASM specifically but can still
choose a different standard.

Given Iroha v2.0 release is imminent, we have one last opportunity to
reconsider the binary standard and this document will detail some
suggestions. While the purpose of this document is not to denigrate
anyone's work with harsh criticism, some of it is appropriate and
shall be levied with as much restraint as is feasible given the
situation.

Without further ado.

* Current situation

The Iroha Special Instructions were originally designed to facilitate
the entirety of operations on the blockchain. As such, the expression
system for instruction arguments, and the instruction set were
designed with maximum flexibility.  However, after 2 years of
development, it was found that even in simple cases the instruction
set is far too cumbersome to be deployed, prompting the need for the
Instruction Set Architecture (ISA) to be restructured.  In this
document we shall assume that the restructuring is conducted with the
most aggressive setting, and that the instruction set is minimised: to
a set of RISC operations consisting of families of assertions and a
unified namespace set instruction.

This ISA, is not particularly expressive, but combined with a
dynamically loaded executor module, which can then define a language
of expressions which are shorthands for the instructions, one can
indeed extend the Iroha language to accept all forms of extensions and
cope with the most complex requests. Additionally, one of the main
concerns of the original design: speed is no longer as big of a
problem. If something is a complex instruction that occurs more than
once, then it is possible to factor it out into its own blockchain
specific instruction, which can then execute as fast as any native
method would in the representation of the executor module.  However,
the speed of execution and the size of the module become more
important. Furthermore, the safety guarantees provided by said module
are an increasing concern and thus the Iroha project faces the
following dilemma.

It is still possible to execute on the original vision with WASM, but
this becomes increasingly problematic.  There are certain design
aspects of this binary standard that preclude its efficient use in the
capacity and with our architecture. Further, the [[https://wiki.hyperledger.org/display/iroha/Scripting+Languages+and+Runtimes+for+Iroha2+Smart+Contracts][original choice]] of
this standard was done under incomplete information, and based on
research with informational gaps and logical fallacies.  While many of
the high-level architectural decisions are sound under the current
situation it is worth reconsidering the original choice.

The comparison was largely between the following standards: WASM,
eBPF, Lua, Rhai and Scilla mentioned only briefly. Aside from these
standards not really being comparable, there was no clear evaluation
of fitness based on a set of criteria: case in point, the pros and
cons for WASM comprised of the following pros:

1. Well known and established in Blockchain space (Polkadot, Kusama,
   other Substrate based networks)
2. Well known and developing fast in web programming space
3. Officially supported by Rust compiler as a target
4. Rust core team is contributing to =wasmtime=
5. It might be required for bridging Substrate based networks
6. We have experience in it within our team and teams closely working to us
7. Developed by wide community
8. Execution can be easily limited with `fuel` (approximate of number
   of assembly instructions that any script is allowed to execute at
   most)

Point 2 has questionable relevance for blockchain smart contract
development.  The only relation is half-tangential in that web
developers (who primarily use other languages), represent a large
demographic of current programmers.

Point 3 is misleading. 32-bit WASM is a Tier 2 target, which to quote
the [[https://doc.rust-lang.org/nightly/rustc/platform-support.html][original documentation]],

#+BEGIN_EXAMPLE
Tier 2 targets can be thought of as "guaranteed to build". The Rust
project builds official binary releases for each tier 2 target, and
automated builds ensure that each tier 2 target builds after each
change. Automated tests are not always run so it's not guaranteed to
produce a working build, but tier 2 targets often work to quite a good
degree and patches are always welcome! For the full requirements, see
Tier 2 target policy in the Target Tier Policy.
#+END_EXAMPLE

in other words, it is one entry in a list of 50 architectures,
including ones like =aarch64-apple-tvos=, =bpfeb-unknown-none=, which
was /not/ mentioned in the eBPF discussion, =loongarch-*= etc.

Point 4, is a logical fallacy. Being good at one thing doesn't
translate into being good at another unrelated thing. It's like
arguing that coffee that was brewed by Picasso is somehow better than
coffee brewed by someone else.

Point 5, neglects to consider the overall architecture of Iroha, and
that even if Iroha uses WASM, because those smart contracts use a
completely different API and ABI and are not modular like pallets in
parity substrate, that then the smart contracts for all intents and
purposes can be considered binary black boxes, which is also true of
any other standard.

Point 6 did not last. Most specialists could equally quickly learn to
work with eBPF and with x86 binaries as well.

Point 7 is a good indicator of health of the project, not of its
suitability for purpose. The community is largely interested in using
WASM to execute logic on the browser that might otherwise require
native support from the architecture.

Point 8 is an extremely minor one, because despite having =fuel= as a
concept, it is not used to limit execution, but instead the even
parity substrate decides weights via a benchmarking process.

The cons are equally underdeveloped.

1. Requires nightly* for the smart contracts themselves
2. Difficult to limit memory further than 3GB in =wasmtime= (needs more
   research)
3. No Interface Types Support - so it has to be simulated through
   passing serialized bytes or mapping memory[fn:6]

The =nightly= build is hardly a drawback.  In practice usage of a
specific nightly tool chain results in improved flexibility.  It is a
minor inconvenience.  Limiting RAM usage to less than 3Gib, is a
non-issue.  It should be possible to monitor the resident RAM taken up
by a smart contract's execution, and kill the process as soon as a
certain threshold is reached.

Point 3 is the only high quality counter-argument, and it is held back
by a verbose technical phrasing, position as last con in a long list,
and a countermeasure that actively makes the problem worse.  Lack of
interface types results in the need to use a serialisation standard to
exchange data.  Given the overall architecture to limit this problem
it is simply enough to expose structures via opaque pointers.

The original documents pitfalls should be avoided when redesigning the
smart contract infrastructure. As a consequence, we shall establish the
set of criteria /before/ discussing and selecting candidates.

* Selection criteria

** Suitability as an executor

*** Fast execution

The executor occupies a much larger role in the execution of regular
instructions.  As such, the standard must come with as little
execution overhead as possible.

Because the executor is unlikely to be upgraded frequently and that
only one executor is active and resident in RAM at any one time, it is
quite possible that optimisations which produce a larger executable
but faster binary are important.

*** Support for static linkage

This follows from fast execution. The ability to eliminate the call
overhead allows one to not deal with an extra boundary and extra
interfaces. Static linkage also allows the executor to be amenable to
link-time optimisation which can, counter-intuitively be used to
reduce the size and execution time of a program simultaneously as
opposed to dynamic linkage.

*** Rust interoperability

The behaviour of the executor is defined in relation to how it
modifies the world state. As such, it makes little sense to write the
executor in a language that is incompatible and incongruous with Rust.

It does not mean that the executor must be written in Rust exactly,
but it ought to be a statically nominative typed language. Rust
itself remains the best option as then the data model entities can be
directly de-structured.

*** Support for efficient Parity SCALE codec operations

Because the binary data is exchanged in this format, the executor is
very likely going to spend most of its time dealing with decoding and
encoding the incoming and out-going data, it should be able to decode
parity SCALE well. This means that it must come from a language in
which SCALE has a native library.

*** Safety and security

The executor comes with some of the most important responsibilities in
the blockchain. It is, in effect, the largest concentration of
potential failure points in the blockchain. As such, it must come with
as strong guarantees as one can muster in order to ensure that many of
the invariants are properly enforced as well as that there is minimal
room for undefined behaviour.

** Suitability for smart contracts

*** Small binary size

This is perhaps the most important consideration of all. Of course,
if an operation is used frequently and it takes up far too much space
in the block store, it might make sense to distribute some of the
functionality as an executor extension.  But given that is not always
a good option, the smart contracts should fit into at most several
kilobytes.

*** Support for dynamic linkage

Regardless of whether the language is able to operate on data model
entities directly or not, it might not be capable of instantiating
objects which behave the same way as other entities of the same type.

As such it is often preferable to manipulate the data model entities
as opaque objects with no known representation and instead rely on the
run-time to provide a dynamic library which can both instantiate these
objects, but also provide a way to interact with them outside the
context of knowing what they are, and instead operating on them
assuming only their public representation.

*** Transparency

While in the case of the executor, one can expect the user to simply
audit the source code of the executor and trust that the process has
reproducible artifacts resulted in a repeatable set of logically
equivalent programs, the same cannot be said of smart contracts.

If the auditing process takes longer and is less convenient than just
looking at the source code, most users are liable not to audit it,
which is antithetical to the entire point of distributed ledger
technology. The more readable the language, the better the chances
that users will be able to spot problematic code on their own, without
requiring complex auditing processes.

*** Sand boxing

Smart contracts must strictly operate as (pure) functions which accept
on-chain parameters as their inputs.  This invariant must be strictly
enforced in order for blocks to be verifiable.  While this is also
true of executors (in whom the sand boxing can be enforced during the
auditing process), it must be mechanically enforced for
smart contracts, because the governance cannot expend resources to
verify that smart contracts are confined.

*** Reproducibility

If the standard is binary then the binary artifact must be easily
reproducible from the source.  The word easily here means that instead
of being a pure Boolean =true/false=, the number of steps needed to
reproduce a build is measured. While it is in principle possible to
make WASM reproducible, the solutions that are produced by
substrate-based blockchains leave much to be desired.

*** Forwards compatibility

A smart contract in a binary format must be made forwards compatible
assuming no major breaking changes in the ABI of the dynamic
libraries.  One cannot assume that the binary standard can always be
recompiled, with very few exceptions: byte-code based platforms, such
as JVM, .NET and JavaScript with just-in-time compilation, the
languages for which ahead-of-time compilation is a requirement cannot
be automatically upgraded to a newer binary standard, unless the
original source code is embedded (which countervails the point about
binary size).

*** Popularity

This point can be a bit controversial, unless clarified.  This is one
of many criteria to be considered, not a prevailing criterion that can
unfairly bump an otherwise useless standard.  Additionally, the
popularity must be understood in context of the application. SQL is
considered the third most popular programming language, but storing
raw SQL statements will hardly result in the same level of
expressiveness as smart contracts in Ethereum.

*** Support

The standard must have a community that is unlikely to abandon the
development of the standard.  This doesn't necessarily mean
decentralised development, as (for example) the =x86_64-win32=
standard, while being developed in-house by Microsoft is unlikely to
be abandoned soon.

*** Compatibility with SDK languages

The Iroha SDK engineers should be able to (if need be), fix their
upstream problems without requiring any help from the Iroha 2 core
team.

*** Ease of maintenance

This is the most important criterion that will determine if having two
different standards for smart contracts and the executor is an ideal
solution. Maintaining two different languages that fit each purpose
well is predicated on the amount of work that could be invested into
the development effort.


** Criteria applied

It should go without saying that the aforementioned criteria should be
applicable to both the executor and smart contracts unless otherwise
specified. We should now establish a baseline and compare a few
popular choices to demonstrate why the more mature criterion-based
approach is better.

*** WASM -- Rust

In terms of execution speed, WASM is not ideal, but it is a temporary
issue.  It cannot be faster to execute than native code, and most of
the cost of running a WASM module is not from the pure execution
standpoint but the overhead of loading and JIT-ting the program, which
can be done in the background, so theoretically WASM satisfies this
criterion with work. Extra precautions taken can reduce the time spent
loading the WASM module before execution, instead focusing on the pure
execution, which is comparable to native programs in =wasmtime=.

WASM is predominantly statically linked. The module system is not yet
developed, as there is no need for it.

WASM has good Rust interoperability.  Rust can be directly compiled
into WASM, and loaded as such.

SCALE is supported well, but comes with a sizeable binary size
overhead. It is worth mentioning that SCALE was designed to be used
from WASM.

WASM on its own has an acceptable security model. Most objects are
confined, but there are no built-in mechanisms for memory
management. The baseline language for WASM: C++, does not have a good
security track record, so while WASM is not a walking security
vulnerability it is not great either. However, WASM with Rust has a
great security model.

WASM has sub-optimal binary size, owing to the fact that it (and Rust
by extension) are predominantly statically linked.  There are multiple
threads written on optimising WASM binary sizes, all of which require
recurring work on behalf of the smart contract author.

WASM does not at present have any support for dynamic linkage. In
addition to the work that was already done to expose the Iroha data
model entities to the C-ABI foreign function interface, additional
work must be done to enable shared-memory dynamic linkage, to reduce
the binary size to an acceptable level.

WASM is opaque. Even the human-readable form of WASM: WAT (TODO link)
is borderline unreadable for users. As such the transparency must be
achieved through reproducibility and an external mechanism for pairing
the code to a piece of source code that must be somehow verifiable.
Unfortunately this is a problematic mechanism as a reproducible build
is often a significant problem, and the person is better-off storing
the source code on chain and compiling from that.

Sand boxing is perhaps the only thing that WASM really excels at. If
the virtual machine for WASM does not enable support for networking,
or access to the system clock, or some other non-deterministic
off-chain processes, the WASM program cannot access them, and thus is
truly sand boxed.

Reproducibility for Rust projects is problematic. It often entails
many sacrifices, like doing the compilation in a single-threaded
manner, running the compilation in a repeatable environment etc.
Getting reproducible builds is still an outstanding task that was not
finished by a senior engineer working on it full-time for four
months. Other languages might not have this problem, but other
languages using Rust would fare much worse on the previous criteria.
If one counts the benefits of Rust and addresses them to WASM, then
one must also count the negatives.

Forwards compatibility is a pain point. Without dynamic linkage WASM
has an unacceptable binary size.  Given this, strict control must be
exercised over the structures, a task which is largely finished.
However a smart contract ought to be JIT-ted to achieve forwards
compatibility of some sort.

Popularity with WASM is difficult to judge. The vast majority of
programming is web development, but it is difficult to judge how much
of it is done via WASM.  The major systems programming languages like
Go, Rust and C++ support WASM very well, but those languages are not
necessarily the best for smart contracts for one reason or another.
In absence of evidence, I would hesitate to argue one way or another
and give WASM the benefit of the doubt.  It is well-represented in
Substrate-based blockchains, even though those are not the vast
majority of blockchains.

Support for WASM can be considered good. As was mentioned in the
original RFC, the Rust core team officially supports =wasmtime=.  How
much of a glowing endorsement that is had been recently tainted by the
behaviour of certain Rust core team members, but as in the previous
case I'd err on the positive side, given that WASM is widely used.

WASM has support that ranges from passable to abysmal. JavaScript by
definition can inter-operate with WASM, but valid JavaScript cannot be
interpreted by =wasmtime=. Because of this, it is extremely unlikely
that JavaScript could be converted into WASM ever[fn:1].  The JVM
languages are nominally supported, but because their runtimes are
typically large, LTO can only eliminate so much code before the smart
contract is untenably large. For the record this problem can be
mitigated with dynamic linkage, but it must also come with a custom
compiler from Java to WASM that excludes the code fragments which
should be linked against externally.  Python is plain too complicated
to be efficiently compiled to WASM.  Additionally the Python that
/can/ be compiled to WASM is much harder to write.

Ease of maintenance is almost non-existent, because the cost of
maintenance had been paid upfront.  This is discounting the fact that
the =data_model= is split into two views, which must be made
compatible in order for one to be available to the host and the other
to be available to the smart contract.

As such WASM is not a terrible standard. it may even be considered an
all-round good candidate if it were in a league of its own and no
other competing standard existed.

*** Native Rust libraries

Native Rust always has an advantage.  The overhead of interpreting a
non-native format is always non-zero, though it can be brought down it
will never be zero, unless a CPU is designed specifically to run WASM
instructions[fn:2]. By some estimations WASM is within margin of error
for benchmarks under some circumstances. I believe that we must assume
that no vector CPU instructions will ever be used in a smart contract,
and all execution must always be single-threaded for this never to be
a problem.

Due to there being no ABI-stability guarantees from =rustc= the Rust
program is typically statically linked against other Rust libraries,
but dynamically linked against the system-wide standard C-library
unless instructed otherwise.

By definition Rust inter-operates with itself.

The reference implementation of the Parity SCALE codec is in Rust.

Rust native has comparable security to Rust with WASM. WASM, in
principle has an advantage, because there are no unsafe operations
which are done via the =libc= interface, and the process is confined
by default, at the same time, undefined behaviour can have the same
level of catastrophic side-effects if it is triggered inside WASM.
Additionally, there's some potential for race conditions because while
Rust can mitigate the issues associated to multi threading, the latter
is completely absent from WASM.  These differences are within margin
of error, because all of the aforementioned drawbacks can easily be
mitigated.  [[https://wiki.archlinux.org/title/Firejail][Firejail]] particularly comes to mind.  In other words, Rust
native libraries has only /slightly/ less security as compared to Rust
compiled to WASM.

Native Rust can be optimised to the same sizes as WASM, potentially
smaller. By that I mean that =libursa.dylib= is =17KiB=, while
=libursa.wasm= is =1.8MiB=.  To be fair neither version of the library
was optimised for size. On the other hand, the WASM is 32-bit which
should have given it an advantage.

Native dynamic linkage was the old standard for Rust and the origin of
the orphan rule.  The dynamic linkage possible between shared objects
is handled by the operating system and is some of the best-tested
forms of dynamic linkage as it dominates the infrastructure.  The work
needed to support =x86_64= dynamic linkage is orders of magnitude less
than the work needed to implement dynamic linkage from the ground up.

Native programs are opaque by definition. Just like with WASM, the
ability to audit smart contracts is contingent on being able to build
smart contracts reproducibly. It does not have WAT, but at the same
time =x86= assembly is far more familiar to programmers.

Sand boxing does not exist by default. In order to sandbox the smart
contract execution environment, one must execute the smart contract
from a confined process, which is easily achievable using
firejail. This is a sub-optimal solution, compared to having a
pessimistic confinement as there is an extra package that must be
fetched which constitutes another dynamic dependency for Iroha which
would not be the case if we used =wasmtime=.

Reproducility in Rust is problematic as in the case of WASM.

Forwards compatibility is guaranteed by the ELF executable format,
provided the dynamic libraries retain their ABI.  In principle,
because none of the system libraries should be linked against, this is
perfect forwards compatibility.

Relative popularity of native code versus WASM is difficult to judge.
However, native code is ubiquitous, as almost all of the web
infrastructure depends on the presence of well-optimised native code.
I am willing to concede that despite several decades of head-start,
the native code might not be orders of magnitude more popular than web
assembly. I am, however unwilling to concede that native code is less
popular owing to factors outlined below.

Support for native code is the best of all binary standards. The
entirety of server and consumer electronics infrastructure must be
replaced in order for native code to be abandoned. WASM is not
well-established at this point and its use is largely situational.

Native support for SDKs varies but is usually as good or better than
with WASM.  JavaScript can not be interpreted without a native
runtime, with that said, in lieu of embedding a full v8 interpreter,
one can use a project like [[https://github.com/vercel/pkg][pkg]] to produce self-contained quasi-native
applications.  This could be done with WASM, but with WASM the
infrastructure to make use of this binary would not be present. The
JVM compatibility is fixed by adding a Java runtime environment, which
can be done once, and used directly.  As an added benefit there are
no special tools needed to produce a compatible =.jar= file and this
process is well-documented. The same goes for any language that
targets native environments and with a natively executable run-time.
The only limiting factor in this instance is the portability of the
run-time and ease of exposing the Iroha native objects to the
run-time, which itself can be made easy with the introduction of an
API in the form of a RISC ISA. Exposing native libraries for dynamic
linkage is a matter of practicality[fn:3], rather than necessity.

Ease of maintenance is less than that of WASM. No run-time is present,
so unless the host system is anything but a POSIX-compliant system
with ELF executables, and the smartcontracts link against non-portable
functions in the standard c library, there is no maintenance to speak
of.

To summarise, a native platform library is by almost all criteria
either a direct upgrade, or a minor improvement. The one instance in
which there is significantly more work to be done is the question of
confinement.  What can generously be called a minor inconvenience is
the fact that running native libraries on heterogeneous networks,
utilising multiple architectures has to come with a separate emulation
layer.  Experience with running =x86= containers on Apple silicone
suggests that this process is not fool-proof[fn:5].  However, it is a price
that is being paid already with WASM, the difference being that none
of the platforms can be considered native and one less executable is
to be shipped.

This approach is also the most general. Agreeing that the
smart contracts and the executor are written in native code for the
host architecture of the nodes, is equivalent to requiring that the
code be a program.  It doesn't even have to be compiled, as exposing
the foreign function interface in such a way that additional
interpreters on the host system can interact with Iroha allows direct
source code storage. The problem of reproducibility can get
exacerbated by this variety, but at the same time an elegant solution
of using a host-side interpreter emerges.

This is merely a proof of concept. Few blockchains utilise a
pre-existing standard, and almost all move towards a domain-specific
ledger-specific virtual machine which was the original intent behind
the Iroha instruction set.  In the interim some standard must serve as
an intermediate language with fast execution and many of the
aforementioned benefits.  The native =x86_64= GNU Linux binary is as
good a temporary standard as WASM. Though, in reality one can do
better.

*** Custom VM

This was the original plan and it is added for the following reason.
The blockchain ledgers that do not come with their own virtual machine
are in the minority.  There are foundational reasons why it is
important and useful to consider this approach for Iroha-based
blockchains.

The ISA restructuring RFC leaves the door open to RISC instructions
which allow logic and Turing-complete programming on-chain.  If WASM
is only a temporary solution there's no need to restrict the Iroha ISA
to a linear programming model.  And logic, jumps and so on can be
added in due course.

For the time being, here's how a custom VM would fare in comparison to
the off-the-shelf implementations.

A custom VM would require considerable amount of work to match the
performance of native logic, it is possible to do so as is
demonstrated with Ethereum, but will require a considerable time
investment.

A custom VM would be difficult to statically link with an
off-the-shelf-linker.  Link-time-optimisation must be implemented
manually, which can result in bloated and inefficient executable
binaries. But this can be done.

A custom VM would have to be modelled after Rust-like concepts  to
have good interoperability.  This implies support for de-structuring,
awareness of the binary layouts, control over Rust ABI.  This is an
intractable task even for Parity Ink.

SCALE is another area where time investment can result in excellent
support, but the investment itself would be monstrous.

A custom VM would have by far the weakest security of all the listed
standards.  A near-copy inherits almost none of the security
guarantees of the original, unless copied in a carefully considered
way.

While it is possible that domain-specific instructions can elide some
implicit information, the proximity of the domain-specific logical
instructions to their native counterparts, directly correlates with
execution speed, so the binary size is at best the same as that of a
natively compiled application for near-native performance.  In
reality, for anything but the simplest operations the binary size
would be orders of magnitude larger than that of the comparably
written native binary, given that every optimisation must be done by
hand.

Dynamic linkage would have to be implemented manually.  This gives the
team the opportunity to tailor the ABI to the domain-specific
concepts, and thus define what is (and crucially isn't) an
ABI-breaking change.  Additionally while this task is monstrous, it is
comparable to the work that would be needed to implement WASM modules.
Unlike the task of WASM modules, there can never be a situation in
which the task is implemented by someone else.

Human-readability would have to be designed in, and will likely
interfere with the other optimisation criteria.  Sand boxing would
similarly need to be designed in.  Reproducible builds from a DSL
would have to be designed in.  Forwards compatibility would be a
constraint that shall always be present.

Popularity would be negligible.  In the eyes of a regular engineer, it
is "just another" instruction set to learn.  Unless it supports
something that the EVM does not, a fair argument can be levied for not
learning the Iroha VM.  Support would have to be provided by the
development team.

SDK support is non-existent. Ease of maintenance is the highest.

The implementation of a custom VM is a monumental task.  It is not
impossible, but it requires a huge investment, and must be done with
careful consideration.  Every performance milestone, must be reached
with internal effort. The reason to do so, usually comes down to
tightening security guarantees, and allowing for simple representation
of domain-specific concepts.  Given that Iroha is instead designed to
have as little domain specific as possible, designing a custom VM
would not achieve much beyond creating a sub-optimal development
experience.

*** eBPF (historical)

This is provided for historical reference, as it was included in the
original discussion.

BPF executes quickly and usually uses statically linked executables.
It has some Rust interoperability, in the sense that BPF packages can
be written in Rust, using [[https://github.com/aya-rs/aya][aya]], a project that sees active development,
but is somewhat backed up with code contributions (40 Pull requests as
of writing). As BPF stands for Berkeley Packet Filtering, the
decoding of the network packets can be done somewhat more efficiently
than normal. In terms of safety and security, BPF actually provides
guarantees by rejecting programs which can crash. This comes at the
cost of a subset of normal programs being accepted, but it is the same
trade-off that is made in the Rust programming language.

BPF would be a good choice for the executor.

The binary size of eBPF was *not measured*. I would expect that eBPF
would offer the same binary size as WASM. eBPF was not designed to
dynamically link in the traditional sense.  It has other mechanisms
such as Attaching and Linking which may not be suitable for reducing
the binary size.  eBPF is opaque. eBPF offers native sand boxing, but
does not preclude applications from accessing off-chain system data,
and thus does not enforce the invariants which are needed.  As such it
offers less sand boxing capability than native libraries do using
firejail.  eBPF builds may or may not be reproducible depending on the
source language and will likely inherit the problems of Rust.  eBPF is
a new standard and while it is made to be maximally backwards and
forwards compatible it is less so than e.g. the ELF standard.

eBPF has niche popularity: there are few users but they consider eBPF
to be indispensable.  Unfortunately, these users also make use of the
low-level features of eBPF, which is problematic for smart contracts.
eBPF has very limited compatibility with SDK languages.  Ease of
maintenance is another problematic aspect. eBPF is not as widely used
as either WASM or native code.

eBPF would be a nice-to-have for the executor, as it would allow to
propagate the performance improvements from the privileged in-kernel
execution to the overall ledger performance, as well as offer
additional mitigation to the elevated responsibilities of the
executor.  It is however a bad choice for smartcontracts as it offers
few if any advantages to using it as a means of sand boxing.  In fact,
eBPF is used to securely run privileged applications, which is the
exact opposite of what smartcontracts should be.

Its inclusion in the discussion is questionable, as at the time, there
were few good reasons to consider eBPF as a standard (because there
was no executor to speak of). It is not a
direct upgrade to WASM.

*** [[https://www.zilliqa.com/language][Scilla]] (historical)

This is also mentioned for historical reasons. The execution speed and
other parameters *have not been bench marked*, Scilla is to be
disqualified on the following three points.
- The language is designed for a [[https://www.zilliqa.com/][Ziliqa]] based network. Adapting it to
  Iroha would require forking the language.
- Ziliqa's exportability to =coq= can be overshadowed by formal proof
  methods available to Rust, and will enforce the smart contracts to
  be constructed with extra work.
- Insider information that is not to be disclosed in a public
  document.

As such Scilla can be disqualified as well.

*** Java virtual machine

While it was mentioned [[Native Rust libraries][previously in the section Rust native libraries]]
that the Java virtual machine can be supported as a subset of
smartcontract languages by exposing a C-ABI foreign function
interface, there are benefits to supporting only the JVM.

Firstly, owing to the maturity and the ubiquity of the standard much
work has gone towards mitigating the performance issues associated to
garbage collection in Java.  [[https://plummerssoftwarellc.github.io/PrimeView/report?id=3990&hi=False&hf=False&hp=False&fi=&fp=&fa=&ff=&fb=&tp=False&sc=pp&sd=True][The top-end procedural Java programs can
compete with mid-level C and C++ programs]].  [[https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/design.html#compiling_loading_and_linking_native_methods][Since Java 8]], it is
possible to link dynamically and statically against native methods:
while this comes with some additional overhead, it would allow C-ABI
to work as a /lingua franca/ but idiomatic error handling via
exceptions would have to be implemented as an extension. This also
answers the question of Rust interoperability: it is possible to do so
by exposing the FFI (the added benefit being that the FFI could be
later used for a different standard, and would have to be implemented
for WASM anyway).  The =iroha-java= library makes use of the Parity
SCALE codec.  This is the first instance where the library is not the
reference implementation so it should be noted that there is
potential for bugs. Fortunately, the SCALE standard is well-specified
and the problem is mitigated.

Java offers memory safety via garbage collection.  Java does not have
special handling for foreign functions and thus it is possible that
the aforementioned functions can themselves cause undefined behaviour.
This is a step down from the guarantees of Rust, and it is possible
that standard and well-tested [[https://www.cisa.gov/news-events/news/apache-log4j-vulnerability-guidance][Java applications can still have
vulnerabilities]].  Not to say that Java applications are insecure,
but the percentage is slightly higher than for Rust applications, to
have a CVE, due to a focus on simplicity of syntax obscuring potential
points of undefined behaviour.

Java binaries need to be bench marked in a representative form to be
comparable.  There are some architectural reasons to suspect that Java
binaries can be smaller than equivalent statically linked Rust
binaries: mainly because most of the standard library is linked
dynamically by the Java Runtime Environment, so one gets most of the
benefit of the dynamic linkage already.  However most objects in the
Iroha data model are defined as Rust structures, so interactions with
them must be done via either intermediate representations, or dynamic
linkage. As such dynamic linkage is a must.  Fortunately, it is easier
to do that with Java than it is with Rust, even though this is at the
cost of performance, most dispatch in Java is dynamic by design, thus
making the question of handling object lifetimes inside libraries much
easier.  Dynamic linkage of some form, thus becomes a necessity.

Java is opaque: it was designed for the open-closed principle, where
classes are open to extension but closed to modification. The same
extends to byte code, because Java was originally designed to
distribute potentially proprietary software.  However, Java byte-code
retains much more information and the decompilation process is capable
of decoding much more information from the byte-code binary than it is
from a native executable.

*** Lua

I must address one mistake mentioned in the [[https://wiki.hyperledger.org/display/iroha/Scripting+Languages+and+Runtimes+for+Iroha2+Smart+Contracts][original discussion]], Lua
was considered slower by virtue of being garbage collected.
[[Well-wtitten][Well-written Lua can]] outperform C++, and is faster than either
JavaScript or Java. Being an interpreted language, the fact that it is
possible to match the performance of C++ and in some situations
outperform it is a testament to the performance of Lua (and
incidentally more than twice as fast as =AssemblyScript=, which
ostensibly compiles to raw WASM). Due to being an interpreted (just in
time compiled) language, Lua has some additional overhead if it is
deployed as a smart contract, a price which one needs to pay once.
Incidentally the same is true of WASM.

In case of Lua static linkage is against other libraries written in
Lua, which is not a great idea. It is possible, but not preferable if
deployed in the executor to instead link dynamically against native
methods to avoid having to compile a larger effective script.

Rust interoperability as with everything above goes through an FFI
layer.  Like with Java, Lua offers a robust standard library which
overlaps with the Iroha data model and some conversion shall be
necessary.  Lua also allows a concept of =userdata= which usually
means a pointer to a raw block of memory that is to be manipulated
opaquely, which lines up perfectly with the intended interaction via
opaque pointers.

Parity SCALE codec structures must be mapped onto Lua native
structures.  However, because Lua supports tables, the conversions can
be carried out purely using dynamic linkage.  This is not as efficient
as static linkage would be if the structures had support for a native
codec, but it would elide the alternative of costly conversions.

Lua is not designed with safety in mind, but it, like Python gains
safety from sanity checks during interpretation.  To trigger undefined
behaviour using Lua, one must cause a problem either with or in the
dynamically linked C-ABI functions.  It must be noted that few
languages can match the safety guarantess of Rust.

Lua does not have a binary size. The source code is what is executed,
because the JIT is relatively quick.  As such, the binary size is
minimal, as the library code often dominates the sizes of the source
code that generated it.  Lua supports native C-ABI dynamic linkage.
Lua is transparent, because the source code is the program.  Lua can
be sand boxed, if the user simply provides functions which do not read
or write the kind of data to be avoided.  Reproducibility is a
non-issue, because the source is the program.  Forwards compatibility
is also a non-issue, because even some changes to signatures and types
which technically break the ABI can be compensated for during the JIT
process.

Lua is well-established as an embedable scripting language.  It is
the basis of the Neovim text editor, and used in multiple high
complexity scripting environments.  Case in point, many of Minecraft's
Turing complete computer demonstrations are actually mainly possible
thanks to Lua. It is well-supported in all of its circles, mature and
well-tested.

The only real drawback to Lua is that if it is accepted as a
blockchain scripting language, then all SDKs must use Lua for their
work. Fortunately the language is easy to learn and easy to debug.
Maintaining a Lua interpreter is as easy as bumping a crate version.
It is only slightly more work than using a native binary executable.

Overall Lua is a virtuous candidate; it can resolve many issues by the
sheer simplicity and ease of use.  The debugging experience is
somewhat less complete, because of lack of robust static analysis
features such as with e.g. Rust, but the language more than makes up
for it, in transparency support and ease of maintenance.

*** Haskell

Haskell is the [[https://plummerssoftwarellc.github.io/PrimeView/report?id=3995&hi=False&hf=False&hp=True&fi=&fp=&fa=&ff=&fb=&tp=False&sc=pp&sd=True][fastest functional programming language]], being only
slower than Rust, Chapel, Zig, C++, C and D, thus being faster than
either Java, JavaScript and Lua, being comparable to languages like
Nim in terms of performance and readability.  Haskell is primarily
statically linked, with a dynamically linked runtime.  What's more
Haskell can both be interpreted (via JIT), or compiled which gives the
blockchain developers some flexibility with respect to execution
methods.

Haskell has certain similarities with Rust, which would make the
interoperability much easier.  Though not seamless, Haskell would be
able to represent any object that Rust can represent.

The only issue is that the support for the Parity Scale Codec is not
built-in and would have to be added by the engineers.

Haskell is the safest choice in this list, because a functional
programming language can set some guarantees that no other method can.
Haskell has a better security track record than even Rust, because
functional programming is fundamentally concerned with side-effects
which includes undefined behaviour, mutation safety, safety from race
conditions, and in some situations -- logic errors[fn:4].

Haskell can produce some of the smallest binaries owing to a
sophisticated runtime that must be present, and provided it uses
dynamic linkage.  Default static linkage even for small programs
produces binaries comparable in size to WASM.  It can also be stored
as source code, and compiled in place.  Owing to ease of composition
and a terse syntax Haskell is used in code golfing.  Haskell does not
have as many complex invariants to uphold as does Rust, thus it can do
dynamic linkage much more simply than Rust.  However, linking
pro-actively, which is to say =dlopen= comes [[https://wiki.haskell.org/Foreign_Function_Interface][with its caveats]]

Transparency requires a Haskell program to be stored as source code.
Haskell binaries are no more intelligible than any other natively
compiled standard.  While interpreted Haskell is nowhere near as
well-optimised as Lua, it is not necessary.

Even if a source-code submission is used the compilation can be done
asynchronously falling back onto interpretation in the cases in which
the compilation was not done sufficiently quickly.  One might think
that this will add complexity, but in fact this system is already in
place to compensate for Web Assembly's design oversights: the output
of =rustc= though unintelligible by humans is still not completely
compiled ahead of time, and must be optimised further.

One more aspect that is unique to Haskell is the ease of
interpretability.  Of the programming languages listed so far only
Haskell (and Scilla) are referential-transparent, meaning that the
meaning of a statement can be replaced with the value of that
statement and in all situations the result of the evaluation will be
completely unchanged.  Identifying bugs in a functional codebase is
much easier.

Sand boxing is one of the key areas where Haskell shines.  All Haskell
programs are purely functional (unlike e.g. OCaml that is functional
by default, but retains an imperative subset).  Any deviations must be
carried through the type system, and unlike =unsafe=[fn:7] in Rust, which
can occur silently inside of functions, functions which can cause
input and output to happen, must carry the information through the
type system.  It is impossible to have anything other than pure
functions without affecting the signature of the =main= function of
the smart contract.

However, unlike WASM it is also impossible to write functions whose
outputs can change randomly.  Randomness, like access to networking
or the file system, is a monad.  Functions which use randomness must
have a =Random= monad attached to their type.  As such Haskell is the
only language on the list that can offer this type of sand boxing, the
first to offer better protection than WASM.

Reproducibility is one area in which source code-only smart contracts
are preferred.  While it is comparatively easy to produce reproducible
binaries, one does not need to do so given that Haskell source code
can be submitted instead of the binary and asynchronously compiled.
Forwards compatibility is also a given for the source code
distribution model. Unfortunately, the Haskell ABI is unstable in the
same way as Rust's, so binaries would not be forwards compatible
unless a particular development practice is employed.

Haskell has a modest but loyal following.  There are not too many
programmers who practice Haskell in their everyday job, but many Rust
programmers are keenly aware of Haskell, and have practiced writing
programs with it.  Haskell is community supported, and
feature-complete.  It is, like C, a language that does not have
fundamental pain points, thus it evolves comparatively little. The
Glasgow Haskell Compiler is the main distribution of Haskell and it is
maintained by the University of Oxford and Microsoft Research.

Haskell does not have good interoperability with any SDK. It is a
fundamentally different programming paradigm.

Maintaining Haskell is usually not difficult owing to the fact that
there are few if any changes for Haskell-internal libraries.  Doing
things the less efficient way, and relying on the compiler to do the
optimisations for declarative programming has the benefit of rarely
having to modify the program.

* Footnotes
[fn:7] It must be said that [[https://hackage.haskell.org/package/base-4.18.0.0/docs/System-IO-Unsafe.html#v:unsafePerformIO][UnsafePerformIO]] is a backdoor to the
monadic model.  It is true that it can obfuscate the presence of
=IO=-associated side-effects.  It is also true that it must be
provided in the runtime, and can be excluded from it.

[fn:6] The [[https://github.com/WebAssembly/component-model][component model]], is still in flux, and [[https://github.com/AmbientRun/Ambient/tree/main][the one project]] that
uses it is in very early stages.  If the status quo approach is
retained, this is definitely an avenue that should be explored instead
of an in-house implementation of the shared memory dynamic linkage.

[fn:5] There is a practice of shipping multi-architecture binaries,
called "fat binaries", and is most notably used in Clear Linux (for a
different purpose, the intend being to ship with native optimisations)
for all the supported =x86_64= variants.  This is not totally
unacceptable; binary size is less of a factor for the executor, and
more of a factor for smart contracts (which can afford slightly worse
performance).

[fn:4] Haskell is, like Scilla is good for formal verification.

[fn:3] For the sake of completeness, let us mention that a =python=
interpreter comes pre-installed with most systems and exposing a
function in a Python-compatible fashion is little more than exposing
it with the C-ABI. For example, the following loads a C-ABI function
that was not specially designed to be callable from python:

#+begin_src python
# ctypes_test.py
import ctypes
import pathlib

if __name__ == "__main__":
	# Load the shared library into ctypes
	libname = pathlib.Path().absolute() / "libcmult.so"
	c_lib = ctypes.CDLL(libname)
#+end_src


[fn:2] A different native format can still beat the regular
native library if e.g. it is run in kernel space and/or in an
operating system free environment with bare-metal

[fn:1] Although in principle JavaScript as a language is not suitable
for financial calculations anyway.
